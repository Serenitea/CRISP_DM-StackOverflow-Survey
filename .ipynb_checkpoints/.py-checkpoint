'''
for years 2016-2019.
processes a a specified column from the raw imported dataframe
'''
def process_col(df, col):
    s = df[col]
    s = s.dropna()
    s_len = s.shape[0]
    cat_count, col_vals = eval_complex_col(df, col)
    s_split = s.str.split(pat = '; ')
    return s,s_len, s_split, cat_count, col_vals


def eval_complex_col(df, col):
    '''
    IN:
    df[col] - every str consists of one or more values (e.g. 'a, b, d')
    
    OUT:
    col_vals - All unique elements found in the column, listed alphabetically
    
    '''
    col_num = df[df[col].isnull() == 0].shape[0]
    col_df = df[col].value_counts().reset_index()
    col_df.rename(columns={'index': col, col:'count'}, inplace = True)
    col_series = pd.Series(col_df[col].unique()).dropna()
    clean_list = col_series.str.split(pat = ';').tolist()
    

    flat_list = []
    for sublist in clean_list:
        for item in sublist:
            flat_list.append(item)
    clean_series = pd.DataFrame(flat_list)
    clean_series[0] = clean_series[0].str.strip()

    col_vals = clean_series[0].unique()
    col_vals = pd.Series(sorted(col_vals))
    cat_count = clean_series[0].value_counts()
    
    
#    print('Unique Categories: ', col_vals)
    return cat_count, col_vals


def clean_df_cat(df, to_remove):
    '''
    Removes columns that match any of the values in the to_remove list

    '''
    for item in to_remove:
        for col in df.columns:
            if item.casefold() == col.casefold():
                df = df.drop(col, axis = 1)
    return df

'''
2016-2019
converts a series of lists into a df with each list as a row
also returns a transposed version.
'''

def s_of_lists_to_df(s):
    df = pd.DataFrame(item for item in s)
    df_transposed = df.transpose()
    return df, df_transposed



def make_df_bool(df, df_transposed, vals_list):
    '''
    for all years (2015-2019)
    creates a df of bool values based on whether each survey response has the value in vals_list.
    df: dataframe of survey responses, 
    vals_list: list of values for conditions of the new columns, with 1 col per val
    '''
    for item in vals_list:
        df[item] = df_transposed.iloc[:,:].isin([item]).sum()
    df_bool = df.loc[:,vals_list]
    return df_bool

'''
Condensed function processing from initial imported df to boolean df
Used for 2017-2019
'''
def process_data(df, col):
    df_droppedna, df_droppedna_len, df_droppedna_split, df_count, df_vals = process_col(df, col)
    df2, df2_transposed = s_of_lists_to_df(df_droppedna_split)
    df_bool = make_df_bool(df2, df2_transposed, df_vals)
    
    return df_bool, df_vals, df_droppedna_len


def process_data_extended(df, col, delimiter):
    '''
    for years 2017-2019.
    processes a specified column from the raw imported dataframe
    for 2017, delimiter = '; '
    for 2018+19, delimiter = ';'
    '''
    s = df[col]
    s = s.dropna()
    df_len = s.shape[0]
    df_count, df_vals = eval_complex_col(df, col)
    s_split = s.str.split(pat = delimiter)

    df_new = pd.DataFrame(item for item in s_split)
    df_new_transposed = df_new.transpose()

    for item in df_vals:
        df_new[item] = df_new_transposed.iloc[:,:].isin([item]).sum()
    df_bool = df_new.loc[:,df_vals]

    return df_bool, df_vals, df_len


def find_other_lang(df):
	'''
	for all years (2015-2019)
	must edit the df to match the lang_comm_list first!
	Finds the difference between the languages of the current year and the list of common languages
	Returns a list of columns to be merged into an "others" column
	'''
    other_lang = set(df.columns).difference(set(lang_comm_list))
    other_lang_list = list(other_lang)
    return other_lang_list

def make_counts2015(ini_df, series_name, index_name):
	'''
	for 2015 data - multiple columns for one data category
	converts a dataframe into a 1-d series to a 2 column df,
	returned df has Columns and column counts
	series sorted alphabetically
	'''
    series = ini_df.count()
    series = series.rename_axis(index_name)
    series = series.rename(series_name).sort_index()
    df = series.reset_index()
#    df.sort_values(by=[sort_by], inplace = True)
    return series, df



def make_counts(ini_df, series_name, index_name):
	'''
	2016-2019
	creates a df of counts for each column of the boolean df
	'''
    series = ini_df.sum()
    series = series.rename_axis(index_name)
    series = series.rename(series_name).sort_index()
    df = series.reset_index()
#    df.sort_values(by=[sort_by], inplace = True)
    return series, df

#Investigate missing data using different thresholds of %NaN missing.
def investigate_nan_threshold(df, interval, start):
    '''
This function finds how many columns have more than a certain percentage 
of data missing.

INPUTS:
    start - the initial threshold percentage of data missing to be analyzed
    interval - the amount of increase in the analysis threshold 
    if the previous threshold has at least 1 column remaining

OUTPUTS:
    Prints the names of the columns that have more than the threshold % of 
    data missing as well as the current threshold.
    '''

    n = start
    df_rows, df_cols = df.shape
    missing_list = [1]
    while len(missing_list) > 0:
        missing_list = [col for col in df.columns if (df[col].isnull().sum()/df_rows)*100 > n]
        if len(missing_list) > 0:
            print('There are {} columns with more than {}% of data missing.'.format(len(missing_list), n))
            print(missing_list)
            print('--------------------------------------')
            n = n+interval
        else:
            return [col for col in df.columns if (df[col].isnull().sum()/df_rows)*100 > n-interval]

def process_data_extended(df, col, delimiter):
    '''
    for years 2017-2019.
    processes a specified column from the raw imported dataframe
    for 2017, delimiter = '; '
    for 2018+19, delimiter = ';'
    IN: df, col, delimiter
    OUT:
    df_bool
    df_vals
    df_len
    '''
    s = df[col]
    s = s.dropna()
    df_len = s.shape[0]
    df_count, df_vals = eval_complex_col(df, col)
    s_split = s.str.split(pat = delimiter)

    df_new = pd.DataFrame(item for item in s_split)
    df_new_transposed = df_new.transpose()

    #
    for item in df_vals:
        df_new[item] = df_new_transposed.iloc[:,:].isin([item]).sum()
    df_bool = df_new.loc[:,df_vals]

    return df_bool, df_vals, df_len


def process_data(df):
    '''
    Input: original 2019 dataframe
    
    Steps:
    1. drop unwanted features
    2. drop rows that don't contain income
    3. drop rows that have more than 45% missing data
    4. convert the mixed variables to bool tables and merge them with the main table
    '''
    row_threshold = 45 #change row threshold here
    
    df = df.drop(feat_drop, axis = 1) #drop unwanted features columns
    df = df[df['ConvertedComp'].isnull() == 0] #drop rows that don't contain income
    df_lowmiss = df[df.isnull().sum(axis=1) < row_threshold].reset_index(drop=True) #drop rows with over 45% missing data

    #remove rows that have more than 140hr worked per week
    df_filtered = df_lowmiss[df_lowmiss['WorkWeekHrs'] < 140]
    #return df_filtered


    for var in dict_dict:
        df_filtered[var].replace(dict_dict[var], inplace = True)

    #convert numerical features to floats
    #df_filtered[num_feat].astype(float)
    for var in num_feat:
        df_filtered[var] = pd.to_numeric(df_filtered[var])

    #make boolean dummies dfs out of mixed categorical features and merge table
    
    merged_df = df_filtered.copy()
    for var in mixed_cat_feat:
        var_bool, var_vals, var_len = process_data_extended(df_filtered, var, ';') #make new bool df with each value having its own column
        #print(var_vals)
        merged_df = pd.DataFrame.merge(merged_df, var_bool, how = 'outer',  left_index = True, right_index = True) #merge new bool df with main df
        merged_df = merged_df.drop(var, axis = 1) #drop the column that was just dummied

    #make dummies out of simple categorical features
    df_dummy = pd.get_dummies(merged_df[cat_feat].dropna().astype(str))
    df_wdummies = merged_df.join(df_dummy, )
    df_wdummies_dropped = df_wdummies.drop(cat_feat, axis = 1)
    
    return df_wdummies_dropped
